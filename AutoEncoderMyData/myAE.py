from keras.layers import Input, Dense, Flatten, Reshape
from keras.models import Model
from keras.regularizers import L1L2

# this is the size of our encoded representations
encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats
lambda_l1 = 0.00001

# this is our input placeholder
input_img = Input(shape=(28, 28, 1))
flat_img = Flatten()(input_img)
# "encoded" is the encoded representation of the input
x = Dense(encoding_dim*3, activation='relu')(flat_img)
x = Dense(encoding_dim*2, activation='relu')(x)
encoded = Dense(encoding_dim, activation='linear', activity_regularizer=L1L2(lambda_l1))(x)

# create a placeholder for an encoded (32-dimensional) input
input_encoded = Input(shape=(encoding_dim,))
x = Dense(encoding_dim*2, activation='relu')(input_encoded)
x = Dense(encoding_dim*3, activation='relu')(x)
flat_decoded = Dense(784, activation='sigmoid')(x)
decoded = Reshape((28, 28, 1))(flat_decoded)

# this model maps an input to its encoded representation
encoder = Model(input_img, encoded)

# create the decoder model
decoder = Model(input_encoded, decoded)

# this model maps an input to its reconstruction
autoencoder = Model(input_img, decoder(encoder(input_img)))

autoencoder.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

from keras.datasets import mnist
import numpy as np
(x_train, _), (x_test, _) = mnist.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test  = np.reshape(x_test,  (len(x_test),  28, 28, 1))
print(x_train.shape)
print(x_test.shape)

test = autoencoder.fit(x_train, x_train,
                epochs=100,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))

encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)

# use Matplotlib (don't ask)
import matplotlib.pyplot as plt

n = 10  # how many digits we will display
plt.figure(figsize=(20, 4))
for i in range(n):
    # display original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # display reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

loss_history = test.history["loss"]
numpy_loss_history = np.array(loss_history)

print(test.history.keys())
# summarize history 
plt.plot(test.history['acc'], c='b', lw=1.5)
plt.plot(test.history['val_acc'], c='r', lw=1.5)
plt.plot(test.history['loss'], c='g', lw=1.5)
plt.plot(test.history['val_loss'], c='m', lw=1.5)

plt.title('model accuracy')
plt.ylabel('loss/accuracy')
plt.xlabel('epoch')
plt.legend(['train acc', 'test acc', 'train loss', 'test loss'], loc='upper left')
plt.tight_layout()
plt.savefig('./result.jpg', format='jpg')
plt.close()